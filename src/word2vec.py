import numpy as np
import itertools
from collections import Counter
import cPickle
import json 
from nltk.tokenize import RegexpTokenizer
import time

tokenizer = RegexpTokenizer(r'\w+')
#not used
STOPWORDS = set(['a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and',
                 'any', 'are', 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below',
                 'between', 'both', 'but', 'by', 'can', 'did', 'do', 'does', 'doing', 'don',
                 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'has', 'have',
                 'having', 'he', 'her', 'here', 'hers', 'herself', 'him', 'himself', 'his', 'how',
                 'i', 'if', 'in', 'into', 'is', 'it', 'its', 'itself', 'just', 'me', 'more', 'most',
                 'my', 'myself', 'no', 'nor', 'not', 'now', 'of', 'off', 'on', 'once', 'only', 'or',
                 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 's', 'same', 'she', 'should',
                 'so', 'some', 'such', 't', 'than', 'that', 'the', 'their', 'theirs', 'them', 'themselves',
                 'then', 'there', 'these', 'they', 'this', 'those', 'through', 'to', 'too', 'under',
                 'until', 'up', 'very', 'was', 'we', 'were', 'what', 'when', 'where', 'which', 'while',
                 'who', 'whom', 'why', 'will', 'with', 'you', 'your', 'yours', 'yourself', 'yourselves'])




def tokenize(text):
    #split on white spaces, remove punctuation, lower case each word
    return [word.lower() for word in tokenizer.tokenize(text)]


def load_data(source_file):

    txts = []
    stars = []
    with open(source_file, 'r') as f:
        for line in f:
            review = json.loads(line)
            try:
                txt = str(review["text"])
                txt = tokenize(txt)
                txts.append(txt)
                stars.append(review["stars"])
                if len(stars)%1000== 0 and len(stars)!=0:
                    print "loaded ", len(stars)," reviews."
            except:
                continue
            
    return (txts, stars)

def build_vocab(sentences):
    """
    Takes in a list of lists of tokens.
    """
    # Build vocabulary
    word_counts = Counter(itertools.chain(*sentences))
    # Mapping from index to word
    vocabulary_inv = [x[0] for x in word_counts.most_common()]
    # Mapping from word to index
    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}
    return [vocabulary, vocabulary_inv]

def vocab_to_word2vec(filename, vocab, k=300):
    """
    Load word2vec from Mikolov
    """
    word_vecs = {}
    with open(filename, "rb") as f:
        header = f.readline()
        vocab_size, layer1_size = map(int, header.split())
        binary_len = np.dtype('float32').itemsize * layer1_size
        for _ in xrange(vocab_size):
            word = []
            while True:
                ch = f.read(1)
                if ch == ' ':
                    word = ''.join(word)
                    break
                if ch != '\n':
                    word.append(ch)
            if word in vocab:
                word_vecs[word] = np.fromstring(f.read(binary_len), dtype='float32')
            else:
                f.read(binary_len)
    
    print str(len(word_vecs))+" words found in word2vec."

    #============================= review sentence filtering, shuold not be needed

    #add unknown words by generating random word vectors
    #count_missing = 0
   
    #for word in vocab:
    #   if word not in word_vecs:
    #       unidentified_words_file.write(word + "\n")
    #       word_vecs[word] = np.random.uniform(-0.25, 0.25, k)
    #       count_missing+=1
    # print str(count_missing)+" words not found, generated by random."

    return word_vecs

def build_word_embedding_mat(word_vecs, vocabulary_inv, k=300):
    """
    Get the word embedding matrix, of size(vocabulary_size, word_vector_size)
    ith row is the embedding of ith word in vocabulary
    """
    vocab_size = len(vocabulary_inv)
    embedding_mat = np.zeros(shape=(vocab_size, k), dtype='float32')
    for idx in xrange(len(vocabulary_inv)):
        embedding_mat[idx] = word_vecs[vocabulary_inv[idx]]
    print "Embedding matrix of size "+str(np.shape(embedding_mat))
    #initialize the first row,
    #embedding_mat[0]=np.random.uniform(-0.25, 0.25, k)
    return embedding_mat


def build_input_data(sentences, labels, vocabulary):
    """
    Maps sentencs and labels to vectors based on a vocabulary.
    """
    x = [[vocabulary[word] for word in sentence] for sentence in sentences]
    y = np.array(labels)
    return [x, y]




# def transform_text(text, vocabulary):
#     """
#     Transform text to list index that can be fed into CNN and LSTM
#     :param text:
#     :param vocabulary: {word: index} mapping
#     :return:
#     """
#     words = tokenize(text).split(' ')
#     print words
#     words = [vocabulary[i] for i in words if i in vocabulary]
#     return words




if __name__ == "__main__":
    print "starting..."
    start_time = time.time()
    #source_file = "../yelp_data/yelp_academic_dataset_review.json"
    source_file = "../yelp_data/new_reviews.json" #pre-filtered reviews
    googlenews_file = "../google_data/GoogleNews-vectors-negative300.bin"
    unidentified_words = "./misc/unidentified_words.txt"

    sentences, labels = load_data(source_file) # sentences - list of list of tokens (words), labels - list of floats
    
    print str(len(sentences)) + " sentences read."
    print "Time elapsed: ", time.time()-start_time, " seconds."
    vocabulary, vocabulary_inv = build_vocab(sentences)
    print "Vocabulary size: "+str(len(vocabulary))
    
    word2vec = vocab_to_word2vec(googlenews_file, vocabulary)

    embedding_mat = build_word_embedding_mat(word2vec, vocabulary_inv) #i-th row corresponds to i-th word in vocab

    x, y = build_input_data(sentences, labels, vocabulary) #for each sentences, convert list of tokes to the list of indices in vocab
    cPickle.dump([x, y, embedding_mat], open('train_mat_filtered1.pkl', 'wb'))
    cPickle.dump(word2vec, open('word2vec1.pkl', 'wb'))
    cPickle.dump(vocabulary, open('vocab_filtered1.pkl', 'wb'))
    cPickle.dump(vocabulary_inv, open('vocab_inv_filtered1.pkl', 'wb'))
    print "Data created"
    print "Time elapsed: ", time.time()-start_time, " seconds."
