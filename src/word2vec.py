import numpy as np
import itertools
from collections import Counter
import cPickle
import numpy as np
import json 
from nltk.tokenize import RegexpTokenizer
import sys
tokenizer = RegexpTokenizer(r'\w+')



def tokenize(text):
    return [word.lower() for word in tokenizer.tokenize(text)]


def load_data(source_file):
    txts = []
    stars = []
    with open(source_file) as f:
        for line in f:
            review = json.loads(line)
            txts.append(tokenize(review['text']))
            stars.append(review["stars"])
    return (txts, stars)

def build_vocab(sentences):
    """
    Takes in a list of lists of words
    Builds a vocabulary mapping from word to index based on the sentences.
    Returns vocabulary mapping and inverse vocabulary mapping.
    """
    # Build vocabulary
    word_counts = Counter(itertools.chain(*sentences))
    # Mapping from index to word
    vocabulary_inv = [x[0] for x in word_counts.most_common()]
    # Mapping from word to index
    vocabulary = {x: i+1 for i, x in enumerate(vocabulary_inv)}
    return [vocabulary, vocabulary_inv]

def vocab_to_word2vec(filename, vocab, k=300):
    """
    Load word2vec from Mikolov
    """
    word_vecs = {}
    with open(filename, "rb") as f:
        header = f.readline()
        vocab_size, layer1_size = map(int, header.split())
        binary_len = np.dtype('float32').itemsize * layer1_size
        for line in xrange(vocab_size):
            word = []
            while True:
                ch = f.read(1)
                if ch == ' ':
                    word = ''.join(word)
                    break
                if ch != '\n':
                    word.append(ch)
            if word in vocab:
               word_vecs[word] = np.fromstring(f.read(binary_len), dtype='float32')
            else:
                f.read(binary_len)
    #print str(len(word_vecs))+" words found in word2vec."

    #add unknown words by generating random word vectors
    count_missing = 0
    for word in vocab:
        if word not in word_vecs:
            word_vecs[word] = np.random.uniform(-0.25, 0.25, k)
            count_missing+=1
    #print str(count_missing)+" words not found, generated by random."
    return word_vecs

def build_word_embedding_mat(word_vecs, vocabulary_inv, k=300):
    """
    Get the word embedding matrix, of size(vocabulary_size, word_vector_size)
    ith row is the embedding of ith word in vocabulary
    """
    vocab_size = len(vocabulary_inv)
    embedding_mat = np.zeros(shape=(vocab_size+1, k), dtype='float32')
    for idx in range(len(vocabulary_inv)):
        embedding_mat[idx+1] = word_vecs[vocabulary_inv[idx]]
    print "Embedding matrix of size "+str(np.shape(embedding_mat))
    #initialize the first row,
    embedding_mat[0]=np.random.uniform(-0.25, 0.25, k)
    return embedding_mat


def build_input_data(sentences, labels, vocabulary):
    """
    Maps sentencs and labels to vectors based on a vocabulary.
    """
    x = [[vocabulary[word] for word in sentence] for sentence in sentences]
    y = np.array(labels)
    return [x, y]


# def transform_text(text, vocabulary):
#     """
#     Transform text to list index that can be fed into CNN and LSTM
#     :param text:
#     :param vocabulary: {word: index} mapping
#     :return:
#     """
#     words = tokenize(text).split(' ')
#     print words
#     words = [vocabulary[i] for i in words if i in vocabulary]
#     return words




if __name__ == "__main__":
    source_file = "../yelp_data/yelp_academic_dataset_review.json"
    googlenews_file = "../google_data/GoogleNews-vectors-negative300.bin"

    sentences, labels = load_data(source_file) #sentences is a list of lists
    print str(len(sentences)) + " sentences read"

    vocabulary, vocabulary_inv = build_vocab(sentences)
    print "Vocabulary size: "+str(len(vocabulary))
    for i in vocabulary.keys()[0:25]:
        print i, vocabulary[i]


    #print transform_text("I Like you, girl! hello...", vocabulary)

    word2vec = vocab_to_word2vec(googlenews_file, vocabulary)

    embedding_mat = build_word_embedding_mat(word2vec, vocabulary_inv)

    print embedding_mat[0:5]
    x, y = build_input_data(sentences, labels, vocabulary)
    cPickle.dump([x, y, embedding_mat], open('train_mat.pkl', 'wb'))
    #cPickle.dump(word2vec, open('../data/word2vec.pkl', 'wb'))
    cPickle.dump(vocabulary, open('vocab.pkl', 'wb'))
    print "Data created"
